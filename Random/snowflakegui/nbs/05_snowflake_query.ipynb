{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp snowflake.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 03_utils_dataframes.ipynb.\n",
      "Converted 04_dstools_preparedata.ipynb.\n",
      "Converted 05_snowflake_query.ipynb.\n",
      "Converted 06_snowflake_copyinto.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``SnowflakeTool:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from easymigration.imports import *\n",
    "from easymigration.utils.parseyaml import *\n",
    "from easymigration import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SnowflakeTool:\n",
    "    \"\"\"\n",
    "    Class that holds basic snowflake functionality including testing connection\n",
    "    and running queries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 sfAccount: str = None,\n",
    "                 sfUser: str = None,\n",
    "                 sfPswd: str = None,\n",
    "                 sfWarehouse: str = None,\n",
    "                 sfDatabase: str = None,\n",
    "                 sfSchema: str = None,\n",
    "                 sfRole: str = None,\n",
    "                 delimiter: str = ';',\n",
    "                 logger = None\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Creates a connection to snowflake account for a simple class\n",
    "        for a python users to take advantage of the computation power of\n",
    "        snowflake. This class is used in all snowflake .py files through\n",
    "        super().\n",
    "\n",
    "        Args:\n",
    "            sfAccount (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            sfUser (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            sfPswd (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            sfWarehouse (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            sfDatabase (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            sfSchema (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            sfRole (str, optional): snowflake credential passed as string. Defaults to None.\n",
    "            delimiter (str, optional): parse delimiter for the query files. Defaults to ';'.\n",
    "            logger ([type], optional): pass custom logger as many libraries are set to Warning. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.engine = create_engine(URL(user=sfUser,\n",
    "                                        password=sfPswd,\n",
    "                                        account=sfAccount,\n",
    "                                        warehouse=sfWarehouse,\n",
    "                                        database=sfDatabase,\n",
    "                                        schema=sfSchema,\n",
    "                                        role=sfRole))\n",
    "        self._logger = logger if logger is not None else logging.getLogger(__name__)\n",
    "        self.delimiter = delimiter\n",
    "        self._logger.info(\"sqlalchemy snowflake engine created\")\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_sql_file(path: str, delimiter: str):\n",
    "        \"\"\"\n",
    "        Helper function to parse string on default delimiter ;\n",
    "        Args:\n",
    "            path (str): path to file that will be parsed\n",
    "            delimiter (str): string type to parse end of sql command\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        single_command = ''\n",
    "        sql_commands = []\n",
    "\n",
    "        for i in lines:\n",
    "            single_command = single_command + i\n",
    "            if f'{delimiter}' in i:\n",
    "                sql_commands.append(single_command)\n",
    "                single_command = ''\n",
    "\n",
    "        return sql_commands\n",
    "\n",
    "    def run_str_query(self, query: str):\n",
    "        \"\"\"\n",
    "        This function will connect to snowflake and run a query that is passed\n",
    "        in as a string.\n",
    "        Args:\n",
    "            query (str): SQL Query String\n",
    "        Returns:\n",
    "            pd.DataFrame/ None: dependent on the query string\n",
    "        \"\"\"\n",
    "\n",
    "        with self.engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    df = pd.read_sql_query(query, self.engine)\n",
    "                    self._logger.info(\"data loaded from snowflake\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "            finally:\n",
    "                self.engine.dispose()\n",
    "                connection.close()\n",
    "                self._logger.info(\"connection to snowflake has been turned off\")\n",
    "        if df.columns[0] == 'status':\n",
    "            self._logger.info(df.status.values[0])\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    def execute_file_query(self, query_path: str):\n",
    "        \"\"\"\n",
    "        Same as ``excecute file``, but with no sql option\n",
    "        might not need this.\n",
    "\n",
    "        Args:\n",
    "            query_path (str): file location to execute\n",
    "        \"\"\"\n",
    "\n",
    "        self._logger.info(\"query being parsed\")\n",
    "        sql_commands = self.parse_sql_file(query_path, self.delimiter)\n",
    "\n",
    "        with self.engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    for i in sql_commands:\n",
    "                        connection.execute(i)\n",
    "                    self._logger.info(\"data loaded from snowflake\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "            finally:\n",
    "                self.engine.dispose()\n",
    "                connection.close()\n",
    "                self._logger.info(\"connection to snowflake has been turned off\")\n",
    "\n",
    "    def execute_file(self, sql: str = None, query_path: str = None):\n",
    "        \"\"\"\n",
    "        will run sql file or txt file that is seperated by self.delimenter\n",
    "        with the option to add a custom sql command if there is something you\n",
    "        want to add to the file being executed\n",
    "\n",
    "        Args:\n",
    "            sql (str, optional): Optional Command to add on top of the file being executed. Defaults to None.\n",
    "            query_path (str, optional): location to sql/txt file to execute. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: returns query results\n",
    "        \"\"\"\n",
    "\n",
    "        self._logger.info(\"query being parsed\")\n",
    "        sql_commands = self.parse_sql_file(query_path, self.delimiter)\n",
    "\n",
    "        with self.engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    for i in sql_commands:\n",
    "                        connection.execute(i)\n",
    "                        self._logger.info(\"sql or txt file excecuted\")\n",
    "                    if sql:\n",
    "                        df = pd.read_sql_query(sql, self.engine)\n",
    "                        self._logger.info(\"data is loaded from snowflake\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "                    df = None\n",
    "                return df\n",
    "\n",
    "            finally:\n",
    "                self.engine.dispose()\n",
    "                connection.close()\n",
    "                self._logger.info(\"connection to snowflake has been turned off\")\n",
    "                \n",
    "    def pandas_to_snowflake(self,\n",
    "                            df: 'pd.DataFrame',\n",
    "                            user: str,\n",
    "                            password: str,\n",
    "                            account: str,\n",
    "                            warehouse: str,\n",
    "                            database: str,\n",
    "                            schema: str,\n",
    "                            table_name: str,\n",
    "                            chunk_size: int = None,\n",
    "                            create_table: bool = False,\n",
    "                            create_statement: str = None,\n",
    "                            parallel: int = 4):\n",
    "        \"\"\"\n",
    "        Function that expect a table has been created as the infer function mis\n",
    "        allocates the size of columns and isn't best practice not chooses the dytpes\n",
    "        of a column, but is much easier to call upon ``infer_to_snowflake`` recommended\n",
    "        when in development.\n",
    "        Args:\n",
    "            df (pd.DataFrame): Dataframe that is going to be sent to snowflake\n",
    "            user (str): snowflake credential passed as string\n",
    "            password (str): snowflake credential passed as string\n",
    "            account (str): snowflake credential passed as string\n",
    "            warehouse (str): snowflake credential passed as string\n",
    "            database (str): snowflake credential passed as string\n",
    "            schema (str): snowflake credential passed as string\n",
    "            table_name (str): Table name being created inside of snowflake\n",
    "            chunk_size (int, optional): Number of elements to be inserted once, if not provided all elements will be dumped once.\n",
    "                                            Defaults to None.\n",
    "            create_table (bool, optional): If true will create a table with create_statement. Defaults to False.\n",
    "            create_statement (str, optional): Give create table statement and the table will\n",
    "                                                be created from this string. Defaults to None.\n",
    "            parallel (int, optional): Number of threads to be used when uploading chunks. Defaults to None.                                                \n",
    "        \"\"\"\n",
    "        _connection = connector.connect(user=os.environ['sfUser'],\n",
    "                                        password=os.environ['sfPswd'],\n",
    "                                        account=os.environ['sfAccount'],\n",
    "                                        warehouse=os.environ['sfWarehouse'],\n",
    "                                        database=os.environ['sfDatabase'],\n",
    "                                        schema=os.environ['sfSchema'])\n",
    "        if create_table:\n",
    "            self.run_str_query(create_statement)\n",
    "        insert_areas = ['INSERT_TABLE_NAME_HERE', 'INSERET_DATABASE_HERE', 'INSERT_SCHEMA_HERE']\n",
    "        inserts = [table_name, database, schema]\n",
    "        table_check = ParseYaml(os.path.join(os.path.abspath(files.__path__[0]), 'snowflake.yaml')).get_yaml(['test']).get('check_table')\n",
    "        for k, v in zip(insert_areas, inserts):\n",
    "            table_check = table_check.replace(k, v)\n",
    "        check = self.run_str_query(table_check)\n",
    "        if check.shape[0] == 0:\n",
    "            raise\n",
    "        try:\n",
    "            df.columns = [x.upper() for x in df.columns]\n",
    "            response, nchunks, nrows, _ = write_pandas(conn=_connection, df=df, table_name=table_name.upper(),\n",
    "                                                       chunk_size=chunk_size, parallel=parallel)\n",
    "            self._logger.info(f'Upload was a success: {response} & the number of rows loaded {nrows}')\n",
    "            if nrows != df.shape[1]:\n",
    "                self._logger.warning(f'Dataframe failed to load {nrows - df.shape[1]}')\n",
    "        except Exception as e:\n",
    "            self._logger.error(f'{e}')\n",
    "            raise\n",
    "        self._logger.info(f'Dataframe uploaded to {database}{table_name}')\n",
    "\n",
    "    def infer_to_snowflake(self,\n",
    "                           df: 'pd.DataFrame',\n",
    "                           table_name: str,\n",
    "                           if_exists: str = 'fail',\n",
    "                           dtype: dict = None,\n",
    "                           chunk_size: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataframe that is going to snowflake\n",
    "            table_name (str): Table name that will be created in snowflake\n",
    "            if_exists (str, optional): How to behave if the table already exists.\n",
    "                                        * fail: Raise a ValueError.\n",
    "                                        * replace: Drop the table before inserting new values.\n",
    "                                        * append: Insert new values to the existing table.\n",
    "                                        Defaults to 'fail'.\n",
    "            dtype (dict, optional): Specifying the datatype for columns. If a dictionary is used, the\n",
    "                                    keys should be the column names and the values should be the\n",
    "                                    SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
    "                                    scalar is provided, it will be applied to all columns.\n",
    "                                    Defaults to None.\n",
    "            chunk_size (int, optional): Specify the number of rows in each batch to be written \n",
    "                                        at a time. By default, all rows will be written at once.\n",
    "                                        Defaults to None.\n",
    "        \"\"\"\n",
    "        df.columns = [x.upper() for x in df.columns]\n",
    "        self._logger.info(f'Begining upload to {table_name}')\n",
    "        df.to_sql(table_name, self.engine.connect(), index=False, chunksize=chunk_size,\n",
    "                  method=pd_writer, if_exists=if_exists)\n",
    "        self._logger.info(f'Dataframe uploaded to {table_name}')\n",
    "\n",
    "    def test_connection(self):\n",
    "        \"\"\"\n",
    "        Tests a connection for snowflake from instantiated object\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    self._logger.info(\"connection to snowflake successful\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "            finally:\n",
    "                self.engine.dispose()\n",
    "                connection.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SnowflakeTool\" class=\"doc_header\"><code>class</code> <code>SnowflakeTool</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SnowflakeTool</code>(**`sfAccount`**:`str`=*`None`*, **`sfUser`**:`str`=*`None`*, **`sfPswd`**:`str`=*`None`*, **`sfWarehouse`**:`str`=*`None`*, **`sfDatabase`**:`str`=*`None`*, **`sfSchema`**:`str`=*`None`*, **`sfRole`**:`str`=*`None`*, **`delimiter`**:`str`=*`';'`*, **`logger`**=*`None`*)\n",
       "\n",
       "Class that holds basic snowflake functionality including testing connection\n",
       "and running queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeTool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = SnowflakeTool(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "assert sf.test_connection() == None, 'anything else the connection has failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from easymigration.utils.parseyaml import *\n",
    "from easymigration.imports import *\n",
    "sf.run_str_query('DROP TABLE IF EXISTS dsdequerytest;')\n",
    "__file__ = './'\n",
    "dict1 = [{'ecid': 150, 'home': 'CA', 'avg_visits': 0.20, 'LTR': 6},\n",
    "         {'ecid': 151, 'home': 'LA', 'avg_visits': 10, 'LTR': 2},\n",
    "         {'ecid': 160, 'home': 'CO', 'avg_visits': 0.56, 'LTR': 4},\n",
    "         {'ecid': 100, 'home': 'LA', 'avg_visits': 2.0, 'LTR': 3}]\n",
    "df = pd.DataFrame(dict1)\n",
    "yaml = ParseYaml(os.path.join(os.path.dirname(__file__), 'files/snowflake.yaml')).get_yaml(['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeTool.infer_to_snowflake\" class=\"doc_header\"><code>SnowflakeTool.infer_to_snowflake</code><a href=\"__main__.py#L222\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeTool.infer_to_snowflake</code>(**`df`**:`DataFrame`, **`table_name`**:`str`, **`if_exists`**:`str`=*`'fail'`*, **`dtype`**:`dict`=*`None`*, **`chunk_size`**:`int`=*`None`*)\n",
       "\n",
       "Args:\n",
       "    df (pd.DataFrame): The dataframe that is going to snowflake\n",
       "    table_name (str): Table name that will be created in snowflake\n",
       "    if_exists (str, optional): How to behave if the table already exists.\n",
       "                                * fail: Raise a ValueError.\n",
       "                                * replace: Drop the table before inserting new values.\n",
       "                                * append: Insert new values to the existing table.\n",
       "                                Defaults to 'fail'.\n",
       "    dtype (dict, optional): Specifying the datatype for columns. If a dictionary is used, the\n",
       "                            keys should be the column names and the values should be the\n",
       "                            SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
       "                            scalar is provided, it will be applied to all columns.\n",
       "                            Defaults to None.\n",
       "    chunk_size (int, optional): Specify the number of rows in each batch to be written \n",
       "                                at a time. By default, all rows will be written at once.\n",
       "                                Defaults to None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeTool.infer_to_snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.pandas_to_snowflake(df=df,\n",
    "                        user=os.environ['sfUser'],\n",
    "                        password=os.environ['sfPswd'],\n",
    "                        account=os.environ['sfAccount'],\n",
    "                        warehouse=os.environ['sfWarehouse'],\n",
    "                        database=os.environ['sfDatabase'],\n",
    "                        schema=os.environ['sfSchema'],\n",
    "                        table_name='dsdequerytest',\n",
    "                        chunk_size = None,\n",
    "                        create_table= True,\n",
    "                        create_statement= yaml.get('create_query_table'))\n",
    "df = sf.run_str_query(\"SELECT * FROM easymigrationtest\")\n",
    "assert df.shape == (4, 4), 'Query 4 observations w/ 4 columns from easymigrationtest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeTool.infer_to_snowflake\" class=\"doc_header\"><code>SnowflakeTool.infer_to_snowflake</code><a href=\"__main__.py#L222\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeTool.infer_to_snowflake</code>(**`df`**:`DataFrame`, **`table_name`**:`str`, **`if_exists`**:`str`=*`'fail'`*, **`dtype`**:`dict`=*`None`*, **`chunk_size`**:`int`=*`None`*)\n",
       "\n",
       "Args:\n",
       "    df (pd.DataFrame): The dataframe that is going to snowflake\n",
       "    table_name (str): Table name that will be created in snowflake\n",
       "    if_exists (str, optional): How to behave if the table already exists.\n",
       "                                * fail: Raise a ValueError.\n",
       "                                * replace: Drop the table before inserting new values.\n",
       "                                * append: Insert new values to the existing table.\n",
       "                                Defaults to 'fail'.\n",
       "    dtype (dict, optional): Specifying the datatype for columns. If a dictionary is used, the\n",
       "                            keys should be the column names and the values should be the\n",
       "                            SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
       "                            scalar is provided, it will be applied to all columns.\n",
       "                            Defaults to None.\n",
       "    chunk_size (int, optional): Specify the number of rows in each batch to be written \n",
       "                                at a time. By default, all rows will be written at once.\n",
       "                                Defaults to None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeTool.infer_to_snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.run_str_query('DROP TABLE IF EXISTS easymigrationtest;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.infer_to_snowflake(df,\n",
    "                      table_name='easymigrationtest',\n",
    "                      if_exists='append')\n",
    "# sf.run_str_query(\"SELECT * FROM dsdequerytest\")\n",
    "df = sf.run_str_query(\"SELECT * FROM easymigrationtest\"); df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (4, 4), 'Query 4 observations w/ 4 columns from easymigrationtest'\n",
    "sf.run_str_query('DROP TABLE IF EXISTS easymigrationtest;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this function is to allow a user to make create a table/View and then query the table with a specific command. This can be done the same way with ``execute_file_query``, but the final command will be a the end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeTool.execute_file\" class=\"doc_header\"><code>SnowflakeTool.execute_file</code><a href=\"__main__.py#L124\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeTool.execute_file</code>(**`sql`**:`str`=*`None`*, **`query_path`**:`str`=*`None`*)\n",
       "\n",
       "will run sql file or txt file that is seperated by self.delimenter\n",
       "with the option to add a custom sql command if there is something you\n",
       "want to add to the file being executed\n",
       "\n",
       "Args:\n",
       "    sql (str, optional): Optional Command to add on top of the file being executed. Defaults to None.\n",
       "    query_path (str, optional): location to sql/txt file to execute. Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    pd.DataFrame: returns query results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeTool.execute_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sf.execute_file(sql='SELECT * FROM easymigrationtest', query_path='testing/view_test.txt')\n",
    "assert df.shape == (10, 25), 'Query 10 observations w/ 25 columns from easymigrationtest'\n",
    "sf.run_str_query('DROP TABLE IF EXISTS easymigrationtest;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar to the above function, but only allows a file to be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeTool.execute_file_query\" class=\"doc_header\"><code>SnowflakeTool.execute_file_query</code><a href=\"__main__.py#L99\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeTool.execute_file_query</code>(**`query_path`**:`str`)\n",
       "\n",
       "Same as ``excecute file``, but with no sql option\n",
       "might not need this.\n",
       "\n",
       "Args:\n",
       "    query_path (str): file location to execute"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeTool.execute_file_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sf.execute_file_query('testing/view_test.txt')\n",
    "sf.run_str_query('DROP TABLE IF EXISTS dsde_test;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeTool.run_str_query\" class=\"doc_header\"><code>SnowflakeTool.run_str_query</code><a href=\"__main__.py#L73\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeTool.run_str_query</code>(**`query`**:`str`)\n",
       "\n",
       "This function will connect to snowflake and run a query that is passed\n",
       "in as a string.\n",
       "Args:\n",
       "    query (str): SQL Query String\n",
       "Returns:\n",
       "    pd.DataFrame/ None: dependent on the query string"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeTool.run_str_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sf.run_str_query(\"SELECT * \\\n",
    "                       FROM easymigrationtest \\\n",
    "                       LIMIT 10;\")\n",
    "assert df.shape == (10, 25), 'Query 10 observations w/ 25 columns from easymigrationtest'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
