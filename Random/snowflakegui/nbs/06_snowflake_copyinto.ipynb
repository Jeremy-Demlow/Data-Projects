{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp snowflake.copyinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 03_utils_dataframes.ipynb.\n",
      "Converted 04_dstools_preparedata.ipynb.\n",
      "Converted 05_snowflake_query.ipynb.\n",
      "Converted 06_snowflake_copyinto.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``CopyInto``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from easymigration.imports import *\n",
    "from easymigration.snowflake.query import *\n",
    "from easymigration.utils.parseyaml import *\n",
    "from easymigration.azure.filehandling import *\n",
    "from easymigration import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CopyInto(SnowflakeTool):\n",
    "\n",
    "    def __init__(self,\n",
    "                 sfAccount: str = None,\n",
    "                 sfUser: str = None,\n",
    "                 sfPswd: str = None,\n",
    "                 sfWarehouse: str = None,\n",
    "                 sfDatabase: str = None,\n",
    "                 sfSchema: str = None,\n",
    "                 sfRole: str = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Instatiation of snowflake mover class inheriting the SnowflakeTool\n",
    "        class from utils.\n",
    "        Args:\n",
    "            sfUser (str, optional): snowflake credential passed as string\n",
    "            sfPswd (str, optional): snowflake credential passed as string\n",
    "            sfWarehouse (str, optional): snowflake credential passed as string\n",
    "            sfDatabase (str, optional): snowflake credential passed as string\n",
    "            sfSchema (str, optional): snowflake credential passed as string\n",
    "            sfRole (str, optional): snowflake credential passed as string\n",
    "            logger ([type], optional): pass custom logger as many libraries are set to Warning. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__(sfAccount,\n",
    "                         sfUser,\n",
    "                         sfPswd,\n",
    "                         sfWarehouse,\n",
    "                         sfDatabase,\n",
    "                         sfSchema,\n",
    "                         sfRole)\n",
    "        self._logger = logger if logger is not None else logging.getLogger(__name__)\n",
    "\n",
    "    def insert_csv(self,\n",
    "                   blob_name: str = None,\n",
    "                   blob_path: str = None,\n",
    "                   storage_account: str = None,\n",
    "                   container_name: str = None,\n",
    "                   table_name: str = None,\n",
    "                   sas_token: str = None,\n",
    "                   fail_on_no_insert: bool = False,\n",
    "                   delimiter: str = ',',\n",
    "                   ):\n",
    "        \"\"\"\n",
    "        Copies a csv file into a given snowflake table from\n",
    "        blob\n",
    "\n",
    "        :param blob_name: name of file in blob\n",
    "        :param blob_path: path to file in blob\n",
    "        :param storage_account: blob storage account name\n",
    "        :param container_name: Azure container ID\n",
    "        :param sas_token: shared access signature token for azure blob\n",
    "        :param table_name: file to get the import data statement\n",
    "        :param delimiter: csv delimeter type\n",
    "\n",
    "        :return response: snowflake response from the copy into statement\n",
    "        \"\"\"\n",
    "\n",
    "        # make blob name here if a path is given\n",
    "        if blob_path:\n",
    "            blob_name = blob_path + '/' + blob_name\n",
    "\n",
    "        # read the sql file from the libary\n",
    "        sql_file = os.path.join(os.path.abspath(files.__path__[0]), 'import_data_csv.sql')\n",
    "        with open(sql_file) as file:\n",
    "            query = ' '.join(file.readlines())\n",
    "        inserts = ['INSERT_AZURE_STORAGE_ACCOUNT_NAME_HERE',\n",
    "                   'INSERT_TABLE_NAME_HERE',\n",
    "                   'INSERT_CONTAINER_NAME_HERE',\n",
    "                   'INSERT_FILE_NAME_HERE',\n",
    "                   'INSERT_AZURE_SAS_TOKEN_HERE',\n",
    "                   'INSERT_DELIMITER_HERE']\n",
    "        insert = [storage_account, table_name, container_name, blob_name, sas_token, delimiter]\n",
    "        for k, v in zip(inserts, insert):\n",
    "            query = query.replace(k, v)\n",
    "        self._logger.info(query)\n",
    "\n",
    "        # execute the snowflake command\n",
    "        response = self.run_str_query(query)\n",
    "\n",
    "        # output query results\n",
    "        self._logger.info('snowflake insertion output:')\n",
    "        self._logger.info(response)\n",
    "\n",
    "        if response.loc[0].status == 'Copy executed with 0 files processed.':\n",
    "            self._logger.info('No files uploaded to snowflake')\n",
    "            if fail_on_no_insert:\n",
    "                raise('fail_on_no_insert was equal TRUE, so program raised error')\n",
    "        else:\n",
    "            # check if the load was executed correctly\n",
    "            assert (response['rows_parsed'][0] == response['rows_loaded'][0]), \\\n",
    "                \"Rows loaded and parsed are not equal\"\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CopyInto\" class=\"doc_header\"><code>class</code> <code>CopyInto</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CopyInto</code>(**`sfAccount`**:`str`=*`None`*, **`sfUser`**:`str`=*`None`*, **`sfPswd`**:`str`=*`None`*, **`sfWarehouse`**:`str`=*`None`*, **`sfDatabase`**:`str`=*`None`*, **`sfSchema`**:`str`=*`None`*, **`sfRole`**:`str`=*`None`*) :: [`SnowflakeTool`](/easymigration/snowflake_query.html#SnowflakeTool)\n",
       "\n",
       "Class that holds basic snowflake functionality including testing connection\n",
       "and running queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CopyInto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_load = CopyInto(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "yaml = ParseYaml('./files/snowflake.yaml').get_yaml(['test'])\n",
    "dict1 = [{'ecid': 150, 'home': 'CA', 'avg_visits': 0.20, 'LTR': 6},\n",
    "         {'ecid': 151, 'home': 'LA', 'avg_visits': 10, 'LTR': 2},\n",
    "         {'ecid': 160, 'home': 'CO', 'avg_visits': 0.56, 'LTR': 4},\n",
    "         {'ecid': 100, 'home': 'LA', 'avg_visits': 2.0, 'LTR': 3}]\n",
    "df = pd.DataFrame(dict1)\n",
    "df.to_csv('./files/test_df.csv', index=False)\n",
    "fh.upload(container_name=yaml.get('container_name'),\n",
    "          file_path='./files/test_df.csv',\n",
    "          dest='snowflake_load_test/test_df.csv',\n",
    "          overwrite=True)\n",
    "_ = sf_load.run_str_query(yaml.get('create_test_table'))\n",
    "sf_load.run_str_query(yaml.get('check_test_table'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CopyInto.insert_csv\" class=\"doc_header\"><code>CopyInto.insert_csv</code><a href=\"__main__.py#L34\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CopyInto.insert_csv</code>(**`blob_name`**:`str`=*`None`*, **`blob_path`**:`str`=*`None`*, **`storage_account`**:`str`=*`None`*, **`container_name`**:`str`=*`None`*, **`table_name`**:`str`=*`None`*, **`sas_token`**:`str`=*`None`*, **`fail_on_no_insert`**:`bool`=*`False`*, **`delimiter`**:`str`=*`','`*)\n",
       "\n",
       "Copies a csv file into a given snowflake table from\n",
       "blob\n",
       "\n",
       ":param blob_name: name of file in blob\n",
       ":param blob_path: path to file in blob\n",
       ":param storage_account: blob storage account name\n",
       ":param container_name: Azure container ID\n",
       ":param sas_token: shared access signature token for azure blob\n",
       ":param table_name: file to get the import data statement\n",
       ":param delimiter: csv delimeter type\n",
       "\n",
       ":return response: snowflake response from the copy into statement"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CopyInto.insert_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = './'\n",
    "sf_load.insert_csv(blob_name=yaml.get('blob_name'),\n",
    "                   blob_path=yaml.get('blob_path'),\n",
    "                   storage_account=yaml.get('account_name'),\n",
    "                   container_name=yaml.get('container_name'),\n",
    "                   table_name=yaml.get('table_name'),\n",
    "                   sas_token=os.environ['snowflake_blob_SAS_token_SECRET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sf_load.run_str_query('SELECT * FROM DSDESNOWFLAKETEST')\n",
    "assert df.shape == (4, 4), 'Query 4 observations w/ 4 columns from easymigration'\n",
    "sf_load.run_str_query('DROP TABLE IF EXISTS easymigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 03_utils_dataframes.ipynb.\n",
      "Converted 04_dstools_preparedata.ipynb.\n",
      "Converted 05_snowflake_query.ipynb.\n",
      "Converted 06_snowflake_copyinto.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
